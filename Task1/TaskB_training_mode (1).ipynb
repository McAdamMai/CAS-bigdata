{"cells":[{"cell_type":"code","execution_count":1,"id":"8123dbe7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35868,"status":"ok","timestamp":1740246139930,"user":{"displayName":"Ha Ha","userId":"04465524217188523514"},"user_tz":300},"id":"8123dbe7","outputId":"a55e0432-3e56-46f6-a5f4-c315243c6cf0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import torch\n","from google.colab import drive\n","drive.mount('/content/drive')\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.transforms import ToTensor\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import pickle\n","import numpy as np\n","\n"]},{"cell_type":"code","execution_count":2,"id":"2ec469d9","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1740246141069,"user":{"displayName":"Ha Ha","userId":"04465524217188523514"},"user_tz":300},"id":"2ec469d9"},"outputs":[],"source":["\n","gnDataset = 3\n","gBase_path = '/content/drive/My Drive/Colab Notebooks/Task2_data/Task2_data'\n","gnClasses = 5\n","gbData_augmentation = True\n","gData_batch_size = 64"]},{"cell_type":"code","execution_count":3,"id":"84518141","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1740246141499,"user":{"displayName":"Ha Ha","userId":"04465524217188523514"},"user_tz":300},"id":"84518141"},"outputs":[],"source":["class CAS771Dataset(Dataset):\n","    def __init__(self, data, labels, transform=None):\n","        self.data = data\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img = self.data[idx]\n","        label = self.labels[idx]\n","        if self.transform and not isinstance(img, torch.Tensor):\n","            img = self.transform(img)\n","        return img, label\n","\n","def _load_data(train_data_path):\n","    raw_data = torch.load(train_data_path)\n","    data = raw_data['data']\n","    labels = raw_data['labels']\n","\n","    # Moves channels to the second dimension\n","    data = data.permute(0, 3, 1, 2)\n","    #print(data.shape)\n","    # Convert labels to a Python list\n","    if isinstance(labels, torch.Tensor):\n","        labels = labels.tolist()\n","\n","    return data, labels\n","\n","def remap_labels(labels, class_mapping):\n","    return [class_mapping[label] for label in labels]\n","\n","def load_class_names(filepath):\n","    with open(filepath, 'r') as file:\n","        classes = [line.strip() for line in file]\n","    return classes\n","def get_data_augmentation(mode):\n","    if gbData_augmentation == False:\n","        return None\n","\n","    if mode == \"train\":\n","        return transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.RandomHorizontalFlip(), # inverse left-right\n","            transforms.RandomRotation(degrees=15), # random rotate\n","            transforms.RandomResizedCrop(32, scale=(0.8, 1.0)), # random crop\n","            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # normalization\n","        ])\n","    else:\n","        return transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","        ])\n","\n"]},{"cell_type":"markdown","id":"1cadcefd","metadata":{"id":"1cadcefd"},"source":["train_data_paths = [f'{gBase_path}/train_dataB_model_{i}.pth' for i in range(1, gnDataset+1)]\n","test_data_paths = [f'{gBase_path}/val_dataB_model_{i}.pth' for i in range(1, gnDataset+1)]\n","print(train_data_paths)\n","print(test_data_paths)\n","\n","classes_path = gBase_path + '/cifar100_classes.txt'\n","classes = load_class_names(classes_path)\n","print(classes)"]},{"cell_type":"code","execution_count":4,"id":"sl7G6wdye60U","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29620,"status":"ok","timestamp":1740246172566,"user":{"displayName":"Ha Ha","userId":"04465524217188523514"},"user_tz":300},"id":"sl7G6wdye60U","outputId":"ddf9660c-3e47-4abc-affc-b35676ba3763"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-4c85dd146545>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  raw_data = torch.load(train_data_path)\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3197, 3, 64, 64]) torch.Size([250, 3, 64, 64])\n","torch.Size([3222, 3, 64, 64]) torch.Size([250, 3, 64, 64])\n","torch.Size([3232, 3, 64, 64]) torch.Size([250, 3, 64, 64])\n"]}],"source":["train_data_paths = [f'{gBase_path}/train_dataB_model_{i}.pth' for i in range(1, gnDataset+1)]\n","test_data_paths = [f'{gBase_path}/val_dataB_model_{i}.pth' for i in range(1, gnDataset+1)]\n","#print(train_data_paths)\n","#print(test_data_paths)\n","\n","for m in range(gnDataset):\n","  tr_data, tr_label= _load_data(train_data_paths[m])\n","  va_data, va_label= _load_data(test_data_paths[m])\n","  print(tr_data.shape, va_data.shape)\n","\n","#classes_path = gBase_path + '/cifar100_classes.txt'\n","#classes = load_class_names(classes_path)\n","#print(classes)"]},{"cell_type":"code","execution_count":5,"id":"1e390ee5","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1740246175920,"user":{"displayName":"Ha Ha","userId":"04465524217188523514"},"user_tz":300},"id":"1e390ee5"},"outputs":[],"source":["def load_data(train_data_path, test_data_path, m=0, save_class_mapping=True):\n","    train_data, train_labels = _load_data(train_data_path)\n","    #print(train_data.shape)\n","    #print(type(train_labels), type(train_labels))\n","    unique_labels = sorted(set(train_labels))\n","    #print(unique_labels)\n","    class_mapping = {label: i for i, label in enumerate(unique_labels)}\n","    #print(f\"Class mapping: {class_mapping}\")\n","\n","\n","    if train_data_path == None:\n","        train_dataloader = None\n","    else:\n","        train_remapped_labels = remap_labels(train_labels, class_mapping)\n","        #print(train_remapped_labels)\n","        transform = get_data_augmentation(\"train\")\n","        train_dataset = CAS771Dataset(train_data, train_remapped_labels, transform=transform)\n","        #print(train_dataset.data.shape,type(train_dataset.labels))\n","\n","        train_dataloader = DataLoader(train_dataset, batch_size=gData_batch_size, shuffle=True, drop_last=False)\n","        #print(train_dataloader)\n","        if save_class_mapping:\n","            # Save the mapping to a file\n","            class_mapping_path = f'{gBase_path}/class_mapping_model_{m+1}.pkl'\n","            with open(class_mapping_path, \"wb\") as f:\n","                pickle.dump(class_mapping, f)\n","\n","    if test_data_path == None:\n","        test_dataloader = None\n","    else:\n","        test_data, test_labels = _load_data(test_data_path)\n","        remapped_test_labels = remap_labels(test_labels, class_mapping)\n","        transform = get_data_augmentation(\"test\")\n","        test_dataset = CAS771Dataset(test_data, remapped_test_labels, transform=transform)\n","        test_dataloader = DataLoader(test_dataset, batch_size=gData_batch_size, shuffle=False, drop_last=False)\n","\n","    return train_dataloader, test_dataloader\n","#     return 1,2"]},{"cell_type":"code","execution_count":6,"id":"bf760ba8","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1740246177201,"user":{"displayName":"Ha Ha","userId":"04465524217188523514"},"user_tz":300},"id":"bf760ba8"},"outputs":[],"source":["# for m in range(gnDataset):\n","#   #print(m)\n","#   train_dataloader, test_dataloader = load_data(train_data_paths[0], test_data_paths[0], 0)\n","#   print(train_dataloader)"]},{"cell_type":"code","execution_count":7,"id":"466692e4","metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1740246178407,"user":{"displayName":"Ha Ha","userId":"04465524217188523514"},"user_tz":300},"id":"466692e4"},"outputs":[],"source":["# batch_size = 64\n","# print(train_dataloader, test_dataloader)"]},{"cell_type":"code","execution_count":8,"id":"4a28cfd8","metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1740246179406,"user":{"displayName":"Ha Ha","userId":"04465524217188523514"},"user_tz":300},"id":"4a28cfd8"},"outputs":[],"source":["class CNNClassifier_1(nn.Module):\n","    def __init__(self, num_classes):\n","        super(CNNClassifier, self).__init__()\n","        self.conv_layers = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            # nn.MaxPool2d(2, 2),\n","\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","        )\n","        self.fc_layers = nn.Sequential(\n","            nn.Linear(256 * 16 * 16, 512),  # FIXED: Corrected Flatten Size\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(512, num_classes)  # Output for 5 classes\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv_layers(x)  # Apply Conv Layers\n","        x = x.view(x.size(0), -1)  # Flatten\n","        return self.fc_layers(x)  # Apply Fully Connected Layers\n","\n"]},{"cell_type":"code","execution_count":9,"id":"UvGvNaFRUgxc","metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1740246181016,"user":{"displayName":"Ha Ha","userId":"04465524217188523514"},"user_tz":300},"id":"UvGvNaFRUgxc"},"outputs":[],"source":["class CNN2(nn.Module):\n","    def __init__(self, num_classes, dropout_rate=0.5):\n","        super(CNN2, self).__init__()\n","\n","        # Convolutional layers\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n","        self.bn3 = nn.BatchNorm2d(256)\n","\n","        # Pooling layers\n","        self.pool1 = nn.MaxPool2d(2)\n","        self.pool2 = nn.MaxPool2d(2)\n","\n","        # Calculate the flattened size by passing dummy input\n","        dummy_input = torch.randn(1, 3, 64, 64)\n","        dummy_output = self.pool2(F.relu(self.bn3(self.conv3(self.pool1(F.relu(self.bn2(self.conv2(F.relu(self.bn1(self.conv1(dummy_input)))))))))))\n","        flattened_size = dummy_output.view(1, -1).size(1)\n","        print(f\"falttened size is {flattened_size}\")\n","\n","        # Fully connected layers\n","        self.fc1 = nn.Linear(flattened_size, 512)\n","        self.fc2 = nn.Linear(512, num_classes)\n","\n","        # Dropout\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, x, return_embedding=False):\n","        # Convolutional layer + ReLU + Batch Normalization\n","        x = F.relu(self.bn1(self.conv1(x)))\n","        x = F.relu(self.bn2(self.conv2(x)))\n","        x = self.pool1(x) # first pooling\n","        x = F.relu(self.bn3(self.conv3(x)))\n","        x = self.pool2(x) # second pooling\n","\n","        # Flatten\n","        x = x.contiguous().view(x.size(0), -1)\n","\n","        # Fully connected layers + Dropout\n","        x = self.dropout(F.relu(self.fc1(x)))\n","\n","        if return_embedding:\n","            return x  # Feature vector ë°˜í™˜\n","\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"code","source":["class CNNClassifier(nn.Module):\n","    def __init__(self, num_classes=5):\n","        super(CNNClassifier, self).__init__()\n","\n","        self.conv_layers = nn.Sequential(\n","            # Conv Block 1\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),  # Added BatchNorm\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),  # Added Pooling here for better downsampling\n","\n","            # Conv Block 2\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),  # Added BatchNorm\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","\n","            # Conv Block 3\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),  # Added BatchNorm\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","        )\n","\n","        # Fully Connected Layers\n","        self.fc_layers = nn.Sequential(\n","            nn.Linear(256 * 8 * 8, 256),  # Reduced size from 512 to 256\n","            nn.ReLU(),\n","            nn.Dropout(0.3),  # Reduced dropout from 0.5 to 0.3\n","            nn.Linear(256, num_classes)  # Output layer\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv_layers(x)  # Apply Conv Layers\n","        x = x.view(x.size(0), -1)  # Flatten before FC layers\n","        return self.fc_layers(x)\n"],"metadata":{"id":"Y3pz_0m-vUSA","executionInfo":{"status":"ok","timestamp":1740246183622,"user_tz":300,"elapsed":18,"user":{"displayName":"Ha Ha","userId":"04465524217188523514"}}},"id":"Y3pz_0m-vUSA","execution_count":10,"outputs":[]},{"cell_type":"code","source":["\n","\n","class ImprovedCNN(nn.Module):\n","    def __init__(self, num_classes=5):\n","        super(ImprovedCNN, self).__init__()\n","\n","        self.conv_layers = nn.Sequential(\n","            # Conv Block 1\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # Extra Conv Layer\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","\n","            # Conv Block 2\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),  # Extra Conv Layer\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","\n","            # Conv Block 3\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # Extra Conv Layer\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","        )\n","\n","        # **Global Average Pooling**\n","        self.gap = nn.AdaptiveAvgPool2d(1)  # Output: (batch_size, 256, 1, 1)\n","\n","        # Fully Connected Layers\n","        self.fc_layers = nn.Sequential(\n","            nn.Linear(256, 128),  # Reduced size\n","            nn.ReLU(),\n","            nn.Dropout(0.3),  # Reduced dropout\n","            nn.Linear(128, num_classes)  # Output layer\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv_layers(x)\n","        x = self.gap(x)  # Apply Global Average Pooling\n","        x = x.view(x.size(0), -1)  # Flatten before FC layers\n","        return self.fc_layers(x)\n"],"metadata":{"id":"DGKi1Nhw6Yxn","executionInfo":{"status":"ok","timestamp":1740246184558,"user_tz":300,"elapsed":18,"user":{"displayName":"Ha Ha","userId":"04465524217188523514"}}},"id":"DGKi1Nhw6Yxn","execution_count":11,"outputs":[]},{"cell_type":"code","execution_count":12,"id":"rmwA7VoU34FQ","metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1740246185733,"user":{"displayName":"Ha Ha","userId":"04465524217188523514"},"user_tz":300},"id":"rmwA7VoU34FQ"},"outputs":[],"source":["def validate_model(model, val_loader,criterion):\n","    #Validate the model on the validation dataset.\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    model.eval()  # Set to evaluation mode\n","\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    # Track class-wise accuracy\n","    class_correct = np.zeros(5)  # Assuming 5 classes\n","    class_total = np.zeros(5)\n","\n","    with torch.no_grad():  # Disable gradient calculation\n","        for imputs, labels in val_loader:\n","            imputs, labels = imputs.to(device), labels.to(device)\n","\n","            # Enable mixed precision (for speedup)\n","            with torch.cuda.amp.autocast():\n","              outputs = model(imputs)\n","              loss = criterion(outputs, labels)  # Compute validation loss\n","\n","            running_loss += loss.item()\n","\n","             # Get predicted class\n","            _, predicted = torch.max(outputs, 1)\n","            # Update correct predictions\n","            correct += (predicted == labels).sum().item()\n","            total += labels.size(0)\n","\n","             # Track class-wise performance\n","            for i in range(len(labels)):\n","                class_total[labels[i].item()] += 1\n","                if predicted[i] == labels[i]:\n","                    class_correct[labels[i].item()] += 1\n","\n","\n","    val_loss = running_loss / len(val_loader)\n","    val_acc = 100 * correct / total\n","\n","    # Compute per-class accuracy\n","    class_acc = 100 * class_correct / (class_total + 1e-6)  # Avoid division by zero\n","\n","    print(f\"\\nâœ… Validation Loss: {val_loss:.4f} | Accuracy: {val_acc:.2f}%\")\n","    print(\"ðŸ“Š Per-Class Accuracy:\")\n","    for i in range(len(class_acc)):\n","        print(f\"  Class {i}: {class_acc[i]:.2f}%\")\n","\n","    return val_loss, val_acc  # Return validation accuracy"]},{"cell_type":"code","execution_count":13,"id":"d88071e5","metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1740246190819,"user":{"displayName":"Ha Ha","userId":"04465524217188523514"},"user_tz":300},"id":"d88071e5"},"outputs":[],"source":["def train_model(model, train_loader, val_loader, num_epochs=50, lr=0.001, patience=5):\n","    #Train a CNN model with validation, learning rate scheduling, and early stopping.\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)  # Add weight decay for regularization\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)  # Reduce LR every 10 epochs\n","\n","    # Track loss and accuracy\n","    train_loss_list = []\n","    train_acc_list = []\n","    val_loss_history = []\n","    val_acc_history = []\n","\n","    best_val_acc = 0.0  # Track best validation accuracy\n","    patience_counter = 0  # Track early stopping condition\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct, total = 0, 0\n","\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","\n","            # Gradient Clipping (Prevents exploding gradients)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","            # Calculate accuracy\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        avg_loss = running_loss / len(train_loader)\n","        accuracy = 100 * correct / total\n","\n","        # Store loss and accuracy for plotting\n","        train_loss_list.append(avg_loss)\n","        train_acc_list.append(accuracy)\n","\n","        # Validate after each epoch\n","        val_loss, val_acc = validate_model(model, val_loader, criterion)\n","\n","        # Store loss and accuracy for plotting\n","        val_loss_history.append(val_loss)\n","        val_acc_history.append(val_acc)\n","\n","        # Print epoch results\n","        print(f\"Epoch {epoch+1}: Train_Loss={avg_loss:.4f}, Train_Accuracy={accuracy:.2f}%,\" f\"Val_Loss: {val_loss:.4f}, Val_Acc: {val_acc:.2f}%\")\n","\n","        # Adjust learning rate\n","        scheduler.step()\n","\n","        # Early Stopping\n","        if val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","            patience_counter = 0  # Reset patience\n","            torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n","        else:\n","            patience_counter += 1  # Increase patience counter\n","\n","        if patience_counter >= patience:\n","            print(f\"Early stopping triggered after {epoch+1} epochs. Best Val Acc = {best_val_acc:.2f}%\")\n","            break\n","    # Load best model before returning\n","    model.load_state_dict(torch.load(\"best_model.pth\"))\n","    print(f\"Training complete. Best validation accuracy: {best_val_acc:.2f}%\")\n","\n","    return model, train_loss_list, train_acc_list, val_loss_history, val_acc_history\n","\n"]},{"cell_type":"code","source":["# def get_plot_1(train_history):\n","#   plt.figure(figsize=(12, 6))\n","\n","#   for name, history in train_history.items():\n","#       plt.plot(history[\"train_loss\"], label=f\"Train Loss - Model {name+1}\")\n","#       plt.plot(history[\"val_loss\"], linestyle=\"dashed\", label=f\"Val Loss - Model {name+1}\")\n","\n","#   plt.xlabel(\"Epochs\")\n","#   plt.ylabel(\"Loss\")\n","#   plt.title(\"Training & Validation Loss of Overlapping Models\")\n","#   plt.legend()\n","#   plt.grid()\n","#   plt.show()"],"metadata":{"id":"cdXRs5ZDxB-6","executionInfo":{"status":"ok","timestamp":1740246191618,"user_tz":300,"elapsed":10,"user":{"displayName":"Ha Ha","userId":"04465524217188523514"}}},"id":"cdXRs5ZDxB-6","execution_count":14,"outputs":[]},{"cell_type":"code","source":["def get_plot(train_history):\n","    plt.figure(figsize=(12, 6))\n","\n","    #epochs = range(1, len(list(train_history.values())[0][\"train_loss\"]) + 1)\n","\n","    for m, history in train_history.items():\n","        # **Plot Loss**\n","        plt.subplot(1, 2, 1)\n","        plt.plot( history[\"train_loss\"], label=f\"Train Loss - Model {m+1}\")\n","        plt.plot( history[\"val_loss\"], linestyle=\"dashed\", label=f\"Validation Loss - Model {m+1}\")\n","        plt.xlabel(\"Epochs\")\n","        plt.ylabel(\"Loss\")\n","        plt.title(\"Training & Validation Loss\")\n","        plt.legend()\n","        plt.grid()\n","\n","        # **Plot Accuracy**\n","        plt.subplot(1, 2, 2)\n","        plt.plot( history[\"train_accuracy\"], label=f\"Train Accuracy - Model {m+1}\")\n","        plt.plot( history[\"val_accuracy\"], linestyle=\"dashed\", label=f\"Validation Accuracy - Model {m+1}\")\n","        plt.xlabel(\"Epochs\")\n","        plt.ylabel(\"Accuracy (%)\")\n","        plt.title(\"Training & Validation Accuracy\")\n","        plt.legend()\n","        plt.grid()\n","\n","    plt.show()\n"],"metadata":{"id":"fvoa5dtuxtks","executionInfo":{"status":"ok","timestamp":1740246192378,"user_tz":300,"elapsed":2,"user":{"displayName":"Ha Ha","userId":"04465524217188523514"}}},"id":"fvoa5dtuxtks","execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":16,"id":"4c25d241","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"4c25d241","executionInfo":{"status":"error","timestamp":1740246265121,"user_tz":300,"elapsed":70844,"user":{"displayName":"Ha Ha","userId":"04465524217188523514"}},"outputId":"b2d93987-6dd4-4d32-93c9-6a8407e28eaa"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-4c85dd146545>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  raw_data = torch.load(train_data_path)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Training Model 1...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-12-e44c8662c545>:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n"]},{"output_type":"stream","name":"stdout","text":["\n","âœ… Validation Loss: 1.7481 | Accuracy: 31.60%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 2.00%\n","  Class 1: 0.00%\n","  Class 2: 12.00%\n","  Class 3: 86.00%\n","  Class 4: 58.00%\n","Epoch 1: Train_Loss=1.4233, Train_Accuracy=40.19%,Val_Loss: 1.7481, Val_Acc: 31.60%\n","\n","âœ… Validation Loss: 1.5995 | Accuracy: 33.20%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 20.00%\n","  Class 1: 4.00%\n","  Class 2: 98.00%\n","  Class 3: 38.00%\n","  Class 4: 6.00%\n","Epoch 2: Train_Loss=1.3064, Train_Accuracy=46.39%,Val_Loss: 1.5995, Val_Acc: 33.20%\n","\n","âœ… Validation Loss: 1.1841 | Accuracy: 52.40%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 52.00%\n","  Class 1: 20.00%\n","  Class 2: 64.00%\n","  Class 3: 74.00%\n","  Class 4: 52.00%\n","Epoch 3: Train_Loss=1.2206, Train_Accuracy=50.92%,Val_Loss: 1.1841, Val_Acc: 52.40%\n","\n","âœ… Validation Loss: 1.3303 | Accuracy: 43.20%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 62.00%\n","  Class 1: 58.00%\n","  Class 2: 28.00%\n","  Class 3: 62.00%\n","  Class 4: 6.00%\n","Epoch 4: Train_Loss=1.1586, Train_Accuracy=55.02%,Val_Loss: 1.3303, Val_Acc: 43.20%\n","\n","âœ… Validation Loss: 1.0932 | Accuracy: 58.80%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 68.00%\n","  Class 1: 26.00%\n","  Class 2: 88.00%\n","  Class 3: 70.00%\n","  Class 4: 42.00%\n","Epoch 5: Train_Loss=1.1103, Train_Accuracy=57.74%,Val_Loss: 1.0932, Val_Acc: 58.80%\n","\n","âœ… Validation Loss: 1.1771 | Accuracy: 58.00%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 60.00%\n","  Class 1: 8.00%\n","  Class 2: 80.00%\n","  Class 3: 96.00%\n","  Class 4: 46.00%\n","Epoch 6: Train_Loss=1.0639, Train_Accuracy=59.15%,Val_Loss: 1.1771, Val_Acc: 58.00%\n","\n","âœ… Validation Loss: 1.1614 | Accuracy: 55.20%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 54.00%\n","  Class 1: 18.00%\n","  Class 2: 88.00%\n","  Class 3: 76.00%\n","  Class 4: 40.00%\n","Epoch 7: Train_Loss=0.9979, Train_Accuracy=61.59%,Val_Loss: 1.1614, Val_Acc: 55.20%\n","\n","âœ… Validation Loss: 1.3855 | Accuracy: 42.40%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 92.00%\n","  Class 1: 26.00%\n","  Class 2: 44.00%\n","  Class 3: 40.00%\n","  Class 4: 10.00%\n","Epoch 8: Train_Loss=0.9618, Train_Accuracy=64.00%,Val_Loss: 1.3855, Val_Acc: 42.40%\n","\n","âœ… Validation Loss: 1.2521 | Accuracy: 51.60%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 24.00%\n","  Class 1: 54.00%\n","  Class 2: 36.00%\n","  Class 3: 62.00%\n","  Class 4: 82.00%\n","Epoch 9: Train_Loss=0.9488, Train_Accuracy=64.65%,Val_Loss: 1.2521, Val_Acc: 51.60%\n","\n","âœ… Validation Loss: 1.4265 | Accuracy: 54.40%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 52.00%\n","  Class 1: 6.00%\n","  Class 2: 48.00%\n","  Class 3: 100.00%\n","  Class 4: 66.00%\n","Epoch 10: Train_Loss=0.9132, Train_Accuracy=66.31%,Val_Loss: 1.4265, Val_Acc: 54.40%\n","Early stopping triggered after 10 epochs. Best Val Acc = 58.80%\n","Training complete. Best validation accuracy: 58.80%\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-13-b2d56a2580a1>:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(\"best_model.pth\"))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Training Model 2...\n","\n","âœ… Validation Loss: 1.1882 | Accuracy: 49.20%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 20.00%\n","  Class 1: 46.00%\n","  Class 2: 74.00%\n","  Class 3: 92.00%\n","  Class 4: 14.00%\n","Epoch 1: Train_Loss=1.3603, Train_Accuracy=42.86%,Val_Loss: 1.1882, Val_Acc: 49.20%\n","\n","âœ… Validation Loss: 1.2320 | Accuracy: 48.40%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 82.00%\n","  Class 1: 46.00%\n","  Class 2: 90.00%\n","  Class 3: 20.00%\n","  Class 4: 4.00%\n","Epoch 2: Train_Loss=1.1980, Train_Accuracy=51.68%,Val_Loss: 1.2320, Val_Acc: 48.40%\n","\n","âœ… Validation Loss: 1.0872 | Accuracy: 56.00%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 18.00%\n","  Class 1: 76.00%\n","  Class 2: 76.00%\n","  Class 3: 32.00%\n","  Class 4: 78.00%\n","Epoch 3: Train_Loss=1.0951, Train_Accuracy=57.05%,Val_Loss: 1.0872, Val_Acc: 56.00%\n","\n","âœ… Validation Loss: 0.8440 | Accuracy: 70.80%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 88.00%\n","  Class 1: 62.00%\n","  Class 2: 84.00%\n","  Class 3: 44.00%\n","  Class 4: 76.00%\n","Epoch 4: Train_Loss=0.9741, Train_Accuracy=63.69%,Val_Loss: 0.8440, Val_Acc: 70.80%\n","\n","âœ… Validation Loss: 1.1758 | Accuracy: 56.80%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 88.00%\n","  Class 1: 20.00%\n","  Class 2: 86.00%\n","  Class 3: 56.00%\n","  Class 4: 34.00%\n","Epoch 5: Train_Loss=0.8497, Train_Accuracy=68.78%,Val_Loss: 1.1758, Val_Acc: 56.80%\n","\n","âœ… Validation Loss: 1.1363 | Accuracy: 62.80%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 20.00%\n","  Class 1: 84.00%\n","  Class 2: 76.00%\n","  Class 3: 66.00%\n","  Class 4: 68.00%\n","Epoch 6: Train_Loss=0.7898, Train_Accuracy=70.86%,Val_Loss: 1.1363, Val_Acc: 62.80%\n","\n","âœ… Validation Loss: 1.0411 | Accuracy: 59.20%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 90.00%\n","  Class 1: 16.00%\n","  Class 2: 58.00%\n","  Class 3: 70.00%\n","  Class 4: 62.00%\n","Epoch 7: Train_Loss=0.7299, Train_Accuracy=73.87%,Val_Loss: 1.0411, Val_Acc: 59.20%\n","\n","âœ… Validation Loss: 0.8462 | Accuracy: 68.80%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 54.00%\n","  Class 1: 88.00%\n","  Class 2: 92.00%\n","  Class 3: 38.00%\n","  Class 4: 72.00%\n","Epoch 8: Train_Loss=0.6680, Train_Accuracy=76.04%,Val_Loss: 0.8462, Val_Acc: 68.80%\n","\n","âœ… Validation Loss: 0.8215 | Accuracy: 69.60%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 82.00%\n","  Class 1: 74.00%\n","  Class 2: 94.00%\n","  Class 3: 62.00%\n","  Class 4: 36.00%\n","Epoch 9: Train_Loss=0.6488, Train_Accuracy=76.54%,Val_Loss: 0.8215, Val_Acc: 69.60%\n","Early stopping triggered after 9 epochs. Best Val Acc = 70.80%\n","Training complete. Best validation accuracy: 70.80%\n","\n","Training Model 3...\n","\n","âœ… Validation Loss: 1.4396 | Accuracy: 36.80%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 46.00%\n","  Class 1: 30.00%\n","  Class 2: 26.00%\n","  Class 3: 30.00%\n","  Class 4: 52.00%\n","Epoch 1: Train_Loss=1.4899, Train_Accuracy=33.85%,Val_Loss: 1.4396, Val_Acc: 36.80%\n","\n","âœ… Validation Loss: 1.4014 | Accuracy: 44.00%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 62.00%\n","  Class 1: 26.00%\n","  Class 2: 26.00%\n","  Class 3: 18.00%\n","  Class 4: 88.00%\n","Epoch 2: Train_Loss=1.3557, Train_Accuracy=43.01%,Val_Loss: 1.4014, Val_Acc: 44.00%\n","\n","âœ… Validation Loss: 1.5807 | Accuracy: 40.00%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 16.00%\n","  Class 1: 38.00%\n","  Class 2: 38.00%\n","  Class 3: 12.00%\n","  Class 4: 96.00%\n","Epoch 3: Train_Loss=1.2750, Train_Accuracy=46.66%,Val_Loss: 1.5807, Val_Acc: 40.00%\n","\n","âœ… Validation Loss: 1.5170 | Accuracy: 40.40%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 28.00%\n","  Class 1: 66.00%\n","  Class 2: 44.00%\n","  Class 3: 50.00%\n","  Class 4: 14.00%\n","Epoch 4: Train_Loss=1.2343, Train_Accuracy=50.31%,Val_Loss: 1.5170, Val_Acc: 40.40%\n","\n","âœ… Validation Loss: 1.8159 | Accuracy: 42.00%\n","ðŸ“Š Per-Class Accuracy:\n","  Class 0: 98.00%\n","  Class 1: 8.00%\n","  Class 2: 12.00%\n","  Class 3: 12.00%\n","  Class 4: 80.00%\n","Epoch 5: Train_Loss=1.1374, Train_Accuracy=54.12%,Val_Loss: 1.8159, Val_Acc: 42.00%\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-0da4cccb9472>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImprovedCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_list\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mval_acc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-b2d56a2580a1>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, lr, patience)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Calculate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["models={}\n","train_history = {}\n","\n","for m in range(gnDataset):\n","    train_dataloader, test_dataloader = load_data(train_data_paths[m], test_data_paths[m], m)\n","    #print(train_dataloader,test_dataloader)\n","    print(f\"\\nTraining Model {m+1}...\")\n","\n","\n","    #model = CNNClassifier(num_classes=5)\n","    model = ImprovedCNN(num_classes=5)\n","\n","    trained_model, loss_list, acc_list, val_loss_list,  val_acc_list = train_model(model, train_dataloader, test_dataloader)\n","\n","    models[m] = trained_model\n","    train_history[m] = {\"train_loss\": loss_list, \"train_accuracy\": acc_list, \"val_loss\": val_loss_list, \"val_accuracy\": val_acc_list}\n","\n","get_plot(train_history)\n"]},{"cell_type":"code","execution_count":null,"id":"bda371b9","metadata":{"id":"bda371b9"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":5}