{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrW9jBcqrvfZ"
   },
   "source": [
    "※ Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "dR19g9Nkrvfc",
    "ExecuteTime": {
     "end_time": "2025-03-18T17:47:17.598339400Z",
     "start_time": "2025-03-18T17:47:17.534491400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "2.6.0+cu118\n",
      "CUDA available: True\n",
      "PyTorch CUDA version:  11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "gnDataset = 3\n",
    "gBase_path = 'C:/Users/AdamBao/PycharmProjects/CAS-bigdata-main/Task2/Task2_data'\n",
    "gnClasses = 5\n",
    "gSaveModelAll = True\n",
    "\n",
    "gDropout_rate = [0.5, 0.5, 0.5]\n",
    "gBatch_momentum = [0.1, 0.1, 0.1]\n",
    "gLearning_rate = [0.00001, 0.00001, 0.00001]\n",
    "gWeight_decay = [1e-3, 1e-3, 1e-3]\n",
    "gOpt_Momentum = [0.1, 0.1, 0.1]\n",
    "\n",
    "gData_batch_size = 32\n",
    "gEarly_stop_patience = 150\n",
    "gnEpochs = 300\n",
    "# for taskB augmentation is not allowed\n",
    "gbData_augmentation = False\n",
    "\n",
    "gModel_names = [\"ResNet10\", \"ResNet10\", \"ResNet10\"] # \"resnet\", \"cnn\"\n",
    "gOptimizer_names = [\"Adam\", \"Adam\", \"Adam\"] # \"Adam\", \"SGD\"\n",
    "gCriterion_names = [\"CrossEntropyLoss\", \"CrossEntropyLoss\", \"CrossEntropyLoss\"]\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"PyTorch CUDA version: \", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKonc2vPrvfe"
   },
   "source": [
    "※ Widget functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "B9HNWXysrvff",
    "ExecuteTime": {
     "end_time": "2025-03-18T17:47:17.672877800Z",
     "start_time": "2025-03-18T17:47:17.550331600Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "\n",
    "class CAS771Dataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform and not isinstance(img, torch.Tensor):\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "def _load_data(train_data_path):\n",
    "    raw_data = torch.load(train_data_path)\n",
    "    data = raw_data['data']\n",
    "    data = data.permute(0, 3, 1, 2)\n",
    "    torch_labels = raw_data['labels']\n",
    "    labels = []\n",
    "    if isinstance(torch_labels[0], torch.Tensor):\n",
    "      labels = [label.item() for label in torch_labels]\n",
    "    return data, labels\n",
    "\n",
    "def remap_labels(labels, class_mapping):\n",
    "    return [class_mapping[label] for label in labels]\n",
    "\n",
    "def load_class_names(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        classes = [line.strip() for line in file]\n",
    "    return classes\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def count_layers(model):\n",
    "    layer_count = 0\n",
    "    for module in model.children():\n",
    "        if not isinstance(module, nn.Dropout):  # deduct Dropout\n",
    "            layer_count += 1\n",
    "    return layer_count\n",
    "\n",
    "\n",
    "class CAS771Plot():\n",
    "    def __init__(self, model, criterion, device, train_dataloader, test_dataloader, model_num):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.train_losses = []\n",
    "        self.validation_losses = []\n",
    "        self.validation_accuracies = []\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.running_loss = 0.0\n",
    "        self.model_num = model_num + 1\n",
    "\n",
    "    def init_running_loss(self):\n",
    "        self.running_loss = 0.0\n",
    "\n",
    "    def add_loss(self, loss):\n",
    "        self.running_loss += loss\n",
    "\n",
    "    def append(self, epoch):\n",
    "        train_loss = self.running_loss / len(self.train_dataloader)\n",
    "        validation_loss, validation_accuracy = self._validate()\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.validation_losses.append(validation_loss)\n",
    "        self.validation_accuracies.append(validation_accuracy)\n",
    "        self._print(epoch, train_loss, validation_loss, validation_accuracy)\n",
    "        return validation_loss\n",
    "\n",
    "    def plot(self):\n",
    "        self._plot_metrics(self.train_losses, self.validation_losses, self.validation_accuracies, self.model_num)\n",
    "\n",
    "    def _print(self, epoch, train_loss, validation_loss, validation_accuracy):\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}\")\n",
    "\n",
    "    def _validate(self):\n",
    "        self.model.eval()  # evaluation mode\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        with torch.no_grad():  # disable gradient calculations\n",
    "            for inputs, labels in self.test_dataloader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_samples += labels.size(0)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        validation_loss = running_loss / len(self.test_dataloader)\n",
    "        validation_accuracy = correct_predictions / total_samples\n",
    "        return validation_loss, validation_accuracy\n",
    "\n",
    "    def _plot_metrics(self, train_losses, validation_losses, validation_accuracies, model_num):\n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
    "        plt.plot(epochs, validation_losses, label='Validation Loss', marker='o')\n",
    "        plt.title(f'Model {model_num} Learning per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, validation_accuracies, label='Validation Accuracy', marker='o', color='green')\n",
    "        plt.title(f'Model {model_num} Accuracy per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class CAS771EarlyStopping():\n",
    "    def __init__(self):\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "\n",
    "    def isStop(self, validation_loss):\n",
    "        if validation_loss < self.best_val_loss: # 검증 손실이 감소하면 best_val_loss 업데이트\n",
    "            self.best_val_loss = validation_loss\n",
    "            self.patience_counter = 0\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "\n",
    "        if self.patience_counter >= gEarly_stop_patience: # 검증 손실이 감소하지 않으면 학습 종료\n",
    "            print(\"Early stopping\")\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "# configurable input_size\n",
    "def get_data_augmentation(mode, input_size=64):\n",
    "    if gbData_augmentation == False:\n",
    "        return None\n",
    "\n",
    "    if mode == \"train\":\n",
    "        return transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomHorizontalFlip(), # inverse left-right\n",
    "            transforms.RandomRotation(degrees=15), # random rotate\n",
    "            transforms.RandomResizedCrop(input_size, scale=(0.8, 1.0)), # random crop\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # normalization\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "def load_data(train_data_path, test_data_path, m=0, save_class_mapping=True):\n",
    "    train_data, train_labels = _load_data(train_data_path)\n",
    "    print(\"Train data shape:\", train_data.shape)\n",
    "    unique_labels = sorted(set(train_labels))\n",
    "    class_mapping = {label: i for i, label in enumerate(unique_labels)}\n",
    "    print(f\"Class mapping: {class_mapping}\")\n",
    "\n",
    "    if train_data_path == None:\n",
    "        train_dataloader = None\n",
    "    else:\n",
    "        train_remapped_labels = remap_labels(train_labels, class_mapping)\n",
    "        transform = get_data_augmentation(\"train\")\n",
    "        train_dataset = CAS771Dataset(train_data, train_remapped_labels, transform=transform)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=gData_batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "        if save_class_mapping:\n",
    "            # Save the mapping to a file\n",
    "            class_mapping_path = f'{gBase_path}/class_mapping_model_{m+1}.pkl'\n",
    "            with open(class_mapping_path, \"wb\") as f:\n",
    "                pickle.dump(class_mapping, f)\n",
    "\n",
    "    if test_data_path == None:\n",
    "        test_dataloader = None\n",
    "    else:\n",
    "        test_data, test_labels = _load_data(test_data_path)\n",
    "        remapped_test_labels = remap_labels(test_labels, class_mapping)\n",
    "        transform = get_data_augmentation(\"test\")\n",
    "        test_dataset = CAS771Dataset(test_data, remapped_test_labels, transform=transform)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=gData_batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzpM59hnrvff"
   },
   "source": [
    "※ Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19314,
     "status": "ok",
     "timestamp": 1739654830124,
     "user": {
      "displayName": "Adam Mak",
      "userId": "09502234756763446111"
     },
     "user_tz": 300
    },
    "id": "4WfEACRtrvfg",
    "outputId": "2d461ecc-bdcc-4f0e-a1d4-41536d174ddd",
    "ExecuteTime": {
     "end_time": "2025-03-18T17:47:17.742109Z",
     "start_time": "2025-03-18T17:47:17.571625600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:/Users/AdamBao/PycharmProjects/CAS-bigdata-main/Task2/Task2_data/train_dataB_model_1.pth', 'C:/Users/AdamBao/PycharmProjects/CAS-bigdata-main/Task2/Task2_data/train_dataB_model_2.pth', 'C:/Users/AdamBao/PycharmProjects/CAS-bigdata-main/Task2/Task2_data/train_dataB_model_3.pth']\n",
      "['C:/Users/AdamBao/PycharmProjects/CAS-bigdata-main/Task2/Task2_data/val_dataB_model_1.pth', 'C:/Users/AdamBao/PycharmProjects/CAS-bigdata-main/Task2/Task2_data/val_dataB_model_2.pth', 'C:/Users/AdamBao/PycharmProjects/CAS-bigdata-main/Task2/Task2_data/val_dataB_model_3.pth']\n",
      "Train data shape: torch.Size([3197, 3, 64, 64])\n",
      "Class mapping: {34: 0, 137: 1, 159: 2, 173: 3, 201: 4}\n"
     ]
    }
   ],
   "source": [
    "train_data_paths = [f'{gBase_path}/train_dataB_model_{i}.pth' for i in range(1, gnDataset+1)]\n",
    "test_data_paths = [f'{gBase_path}/val_dataB_model_{i}.pth' for i in range(1, gnDataset+1)]\n",
    "print(train_data_paths)\n",
    "print(test_data_paths)\n",
    "m = 0\n",
    "train_dataloader, test_dataloader = load_data(train_data_paths[m], test_data_paths[m], m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6FUpdLnrvfh"
   },
   "source": [
    "※ Models"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "MobileNetV2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self,num_classes,pretrained=False,use_small_input=False,dropout_rate=False):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        if pretrained:\n",
    "            self.model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "        else:\n",
    "            self.model = models.mobilenet_v2(weights=None)\n",
    "        # 2) Modify for small input if requested (e.g., 64x64)\n",
    "        if use_small_input:\n",
    "            self.model.features[0][0].stride = (1, 1)  # originally (2, 2)\n",
    "        # 3) Modify the classifier\n",
    "        in_features = self.model.last_channel  # typically 1280 for MobileNetV2\n",
    "        self.model.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-18T17:47:17.742109Z",
     "start_time": "2025-03-18T17:47:17.636759500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ResNet50"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=False, use_small_input=False, dropout_rate=False):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        if pretrained:\n",
    "            self.model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        else:\n",
    "            self.model = models.resnet50(weights=None)\n",
    "        # 2) Modify the first layer for small inputs if desired\n",
    "        if use_small_input:\n",
    "            # Example: kernel_size=3, stride=1 for 64x64\n",
    "            self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
    "            )\n",
    "            self.model.bn1 = nn.BatchNorm2d(64)\n",
    "            self.model.layer2[0].conv2.stride(1,1)\n",
    "            if self.model.layer2[0].downsample is not None:\n",
    "                self.model.layer2[0].downsample[0].stride = (1,1)\n",
    "        # 3) Modify the final fully connected layer\n",
    "        in_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-18T17:47:17.742633800Z",
     "start_time": "2025-03-18T17:47:17.646151900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfeZUr6prvfh"
   },
   "source": [
    "1. ResNet1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "# later add this back \n",
    "        #self.block3 = BasicBlock(128, 256, stride=2)  # (Layers #6,7)\n",
    "        #self.block4 = BasicBlock(256, 256, stride=2)  # (Layers #8,9)\n",
    "        #self.final_conv = nn.Conv2d(256, 384, kernel_size=1, stride=1, bias=False)\n",
    "        #self.final_bn = nn.BatchNorm2d(384)\n",
    "class ResNet10Wide(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=gDropout_rate):\n",
    "        super(ResNet10Wide, self).__init__()\n",
    "        # it used to be kernel = 3, stride = 1, padding = 1\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5,stride=1, padding=2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.block1 = BasicBlock(64, 64, stride=1)    # (Layers #2,3)\n",
    "        self.block2 = BasicBlock(64, 128, stride=2)   # (Layers #4,5)\n",
    "        self.final_conv = nn.Conv2d(128, 256, kernel_size=1, stride=1, bias=False)\n",
    "        self.final_bn = nn.BatchNorm2d(256)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        dummy_input = torch.randn(1, 3, 64, 64)\n",
    "        dummy_output = self._forward_features(dummy_input)\n",
    "        flattened_size = dummy_output.view(-1).size(0)\n",
    "        print(f\"[ResNet10Wide] Flattened size: {flattened_size}\")\n",
    "        self.fc1 = nn.Linear(flattened_size, 512)   # Extra FC\n",
    "        #self.fc2 = nn.Linear(512, 512)             # Another dense layer\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def _forward_features(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        #x = self.block4(x)\n",
    "        x = F.relu(self.final_bn(self.final_conv(x)))\n",
    "        x = self.global_pool(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, return_embedding=False):\n",
    "        x = self._forward_features(x)\n",
    "        x = x.view(x.size(0), -1)       # Flatten\n",
    "        x = F.relu(self.fc1(x))         # FC1\n",
    "        x = self.dropout(x)             # Dropout\n",
    "        #x = F.relu(self.fc2(x))         # FC2\n",
    "        if return_embedding:\n",
    "            return x                    # For feature extraction\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)          # Final classifier\n",
    "        return x\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        # 1st conv layer of the block\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride,padding=1,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        # 2nd conv layer of the block\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        # Shortcut if shape mismatch (stride != 1 or channel change)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)  # Skip connection\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "class ResNet10(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=gDropout_rate):\n",
    "        super(ResNet10, self).__init__()\n",
    "        # 1) Initial Convolution (Conv Layer #1)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2,padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # 2) Four Residual Blocks\n",
    "        self.block1 = BasicBlock(64, 64, stride=1)   # (Conv Layers #2,3)\n",
    "        self.block2 = BasicBlock(64, 128, stride=2)   # (Conv Layers #4,5)\n",
    "        self.block3 = BasicBlock(128, 256, stride=2)  # (Conv Layers #6,7)\n",
    "        self.block4 = BasicBlock(256, 256, stride=2) # (Conv Layers #8,9)\n",
    "        self.final_conv = nn.Conv2d(256, 384, kernel_size=1, stride=1, bias=False)\n",
    "        self.final_bn = nn.BatchNorm2d(384)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        dummy_input = torch.randn(1, 3, 64, 64)\n",
    "        dummy_output = self._forward_features(dummy_input)\n",
    "        flattened_size = dummy_output.view(-1).size(0)\n",
    "        self.fc1 = nn.Linear(flattened_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.cls = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def _forward_features(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = F.relu(self.final_bn(self.final_conv(x)))\n",
    "        x = self.global_pool(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, return_embedding=False):\n",
    "        x = self._forward_features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if return_embedding:\n",
    "            return x  # Return feature vector if requested\n",
    "        x = self.dropout(x)\n",
    "        x = self.cls(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-18T17:47:17.743697300Z",
     "start_time": "2025-03-18T17:47:17.650227600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AktRzt33rvfi"
   },
   "source": [
    "2. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "8IvYILuarvfi",
    "ExecuteTime": {
     "end_time": "2025-03-18T17:47:17.745370500Z",
     "start_time": "2025-03-18T17:47:17.664508Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "gnGPU = \"0\"\n",
    "gnDataset = 3\n",
    "gBase_path = 'CAS771/Task1_data'\n",
    "gnClasses = 5\n",
    "gDropout_rate = 0.9\n",
    "gBatch_momentum = 0.1\n",
    "gLearning_rate = 0.00001\n",
    "gWeight_decay = 1e-3\n",
    "gOpt_Momentum = 0.9\n",
    "gData_batch_size = 32\n",
    "gEarly_stop_patience = 15\n",
    "gnEpochs = 200\n",
    "gbData_augmentation = False\n",
    "\n",
    "'''\n",
    "class ImprovedCNN(nn.Module):\n",
    "    def __init__(self, num_classes,dropout_rate=0.5):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Conv Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            #nn.Conv2d(64, 64, kernel_size=3, padding=1),  # Extra Conv Layer\n",
    "            #nn.BatchNorm2d(64),\n",
    "            #nn.ReLU(),\n",
    "            #nn.MaxPool2d(2, 2),\n",
    "            # Conv Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),  # Extra Conv Layer\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # Conv Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # Extra Conv Layer\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        # **Global Average Pooling*\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)  # Output: (batch_size, 256, 1, 1)\n",
    "        # Fully Connected Layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(256, 128),  # Reduced size\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.7),  # Reduced dropout\n",
    "            nn.Linear(128, num_classes)  # Output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv_layers(x)\n",
    "\n",
    "        x = self.gap(x)  # Apply Global Average Pooling\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten before FC layers\n",
    "\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-zNcL0Arvfj"
   },
   "source": [
    "3. CNN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "X2CFYQA7rvfj",
    "ExecuteTime": {
     "end_time": "2025-03-18T17:47:17.745899500Z",
     "start_time": "2025-03-18T17:47:17.672877800Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "gnGPU = \"0\"\n",
    "gnDataset = 3\n",
    "gBase_path = 'CAS771/Task1_data'\n",
    "gnClasses = 5\n",
    "gDropout_rate = 0.5\n",
    "gBatch_momentum = 0.1\n",
    "gLearning_rate = 0.00001\n",
    "gWeight_decay = 1e-3\n",
    "gOpt_Momentum = 0.9\n",
    "gData_batch_size = 64\n",
    "gEarly_stop_patience = 15\n",
    "gnEpochs = 300\n",
    "gbData_augmentation = False\n",
    "\n",
    "'''\n",
    "class CNN2(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate):\n",
    "        super(CNN2, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        #self.conv1_extra = nn.Conv2d(64, 64, kernel_size=5, padding=1)\n",
    "        #self.bn1_extra = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(32,32, kernel_size=5, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        #self.conv2_extra = nn.Conv2d(128, 128, kernel_size=5, padding=1)\n",
    "        #self.bn2_extra = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=7, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        #self.conv3_extra = nn.Conv2d(256, 256, kernel_size=7, padding=1)\n",
    "        #self.bn3_extra = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Pooling layers\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        # Calculate the flattened size by passing dummy input\n",
    "        dummy_input = torch.randn(1, 3, 64, 64)\n",
    "        dummy_output = self.pool2(F.relu(self.bn3(self.conv3(self.pool1(F.relu(self.bn2(self.conv2(F.relu(self.bn1(self.conv1(dummy_input)))))))))))\n",
    "        flattened_size = dummy_output.view(1, -1).size(1)\n",
    "        print(f\"falttened size is {flattened_size}\")\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(flattened_size, 32)\n",
    "        self.fc2 = nn.Linear(32, num_classes)\n",
    "        \"\"\"self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(256, 128),  # Reduced size\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Reduced dropout\n",
    "            nn.Linear(128, num_classes)  # Output layer\n",
    "        )\"\"\"\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, return_embedding=False):\n",
    "        # Convolutional layer + ReLU + Batch Normalization\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "       # x = F.relu(self.bn1_extra(self.conv1_extra(x)))\n",
    "      #  x = self.pool1(x) # second pooling\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        #x = F.relu(self.bn2_extra(self.conv2_extra(x)))\n",
    "        x = self.pool2(x) # first pooling\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        #x = F.relu(self.bn3_extra(self.conv3_extra(x)))\n",
    "        x = self.pool3(x) # second pooling\n",
    "        #x = self.gap(x)\n",
    "        #x = x.view(x.size(0), -1)  # Flatten before FC layers\n",
    "        #return self.fc_layers(x)\n",
    "        # Flatten\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers + Dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "\n",
    "        if return_embedding:\n",
    "            return x  # Feature vector 반환\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_85fVs-Crvfj"
   },
   "source": [
    "※ Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "Q5s_Gs0Zrvfj",
    "ExecuteTime": {
     "end_time": "2025-03-18T17:47:17.746445300Z",
     "start_time": "2025-03-18T17:47:17.683593100Z"
    }
   },
   "outputs": [],
   "source": [
    "def _get_model(model_name, dropout_rate, batch_momentum, m = 0):\n",
    "    if model_name == \"cnn\":\n",
    "        model = ImprovedCNN(num_classes=gnClasses,\n",
    "                     dropout_rate=dropout_rate).to(device)\n",
    "    elif model_name == \"cnn2\":\n",
    "        model = CNN2(num_classes=gnClasses,\n",
    "                     dropout_rate=dropout_rate).to(device)\n",
    "    elif model_name == \"ResNet10\":\n",
    "        model = ResNet10Wide(num_classes=gnClasses,\n",
    "                     dropout_rate=dropout_rate).to(device)\n",
    "    elif  model_name == \"ResNet50\":\n",
    "        model = ResNet50(num_classes=gnClasses,\n",
    "                     dropout_rate=dropout_rate).to(device)\n",
    "\n",
    "    print(f\"Model{m+1} parameters: {count_parameters(model)}\")\n",
    "    print(f\"Model{m+1} Total layers: {count_layers(model)}\")\n",
    "    return model\n",
    "\n",
    "def _get_optimizer(opname, model, learning_rate, weight_decay, opt_momentum):\n",
    "    if opname == \"Adam\":\n",
    "        return optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif opname == \"SGD\":\n",
    "        return optim.SGD(model.parameters(), lr=learning_rate, momentum = opt_momentum, weight_decay=weight_decay)\n",
    "\n",
    "def _get_criterion(criterion_name):\n",
    "    if criterion_name == \"CrossEntropyLoss\":\n",
    "        return nn.CrossEntropyLoss()\n",
    "    return nn.CrossEntropyLoss()\n",
    "\n",
    "def get_models(model_names, optimizer_names, criterion_names):\n",
    "    models = []\n",
    "    optimizers = []\n",
    "    criterions = []\n",
    "    for m in range(gnDataset):\n",
    "        models.append(_get_model(model_names[m], gDropout_rate[m], gBatch_momentum[m], m))\n",
    "        optimizers.append(_get_optimizer(optimizer_names[m], models[m], gLearning_rate[m], gWeight_decay[m], gOpt_Momentum[m]))\n",
    "        criterions.append(_get_criterion(criterion_names[m]))\n",
    "\n",
    "    return models, optimizers, criterions\n",
    "\n",
    "\n",
    "def save_model(model, m=0, all=False):\n",
    "    if all == True:\n",
    "        model_saved_path = f\"{gBase_path}/model{m+1}_all.pth\"\n",
    "        torch.save(model, model_saved_path)\n",
    "    else:\n",
    "        model_saved_path = f\"{gBase_path}/model{m+1}_weights.pth\"\n",
    "        torch.save(model.state_dict(), model_saved_path)\n",
    "\n",
    "    print(f\"Model{m+1} is saved to: {model_saved_path}\")\n",
    "\n",
    "def load_model(model, m=0, all=False):\n",
    "    if all == True:\n",
    "        model_saved_path = f\"{gBase_path}/model{m+1}_all.pth\"\n",
    "        model.load(model_saved_path)\n",
    "    else:\n",
    "        model_saved_path = f\"{gBase_path}/model{m+1}_weights.pth\"\n",
    "        model.load_state_dict(torch.load(model_saved_path))\n",
    "\n",
    "    print(f\"Model{m+1} is loaded from: {model_saved_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWi1Xt_Zrvfk"
   },
   "source": [
    "※ Training / Validating for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-M1YYq8Jrvfk",
    "outputId": "4b89b624-d4bf-405c-8055-ea7004c798cc",
    "ExecuteTime": {
     "start_time": "2025-03-18T17:47:17.693204600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResNet10Wide] Flattened size: 256\n",
      "Model1 parameters: 476485\n",
      "Model1 Total layers: 9\n",
      "[ResNet10Wide] Flattened size: 256\n",
      "Model2 parameters: 476485\n",
      "Model2 Total layers: 9\n",
      "[ResNet10Wide] Flattened size: 256\n",
      "Model3 parameters: 476485\n",
      "Model3 Total layers: 9\n",
      "Train data shape: torch.Size([3197, 3, 64, 64])\n",
      "Class mapping: {34: 0, 137: 1, 159: 2, 173: 3, 201: 4}\n",
      "Epoch 1, Train Loss: 1.6002, Validation Loss: 1.5593, Validation Accuracy: 0.4240\n",
      "Epoch 2, Train Loss: 1.5555, Validation Loss: 1.5191, Validation Accuracy: 0.4120\n",
      "Epoch 3, Train Loss: 1.5200, Validation Loss: 1.4859, Validation Accuracy: 0.4120\n",
      "Epoch 4, Train Loss: 1.4949, Validation Loss: 1.4584, Validation Accuracy: 0.4000\n",
      "Epoch 5, Train Loss: 1.4721, Validation Loss: 1.4387, Validation Accuracy: 0.4320\n",
      "Epoch 6, Train Loss: 1.4545, Validation Loss: 1.4235, Validation Accuracy: 0.4520\n",
      "Epoch 7, Train Loss: 1.4366, Validation Loss: 1.4083, Validation Accuracy: 0.4360\n",
      "Epoch 8, Train Loss: 1.4200, Validation Loss: 1.3930, Validation Accuracy: 0.4600\n",
      "Epoch 9, Train Loss: 1.4190, Validation Loss: 1.3829, Validation Accuracy: 0.4400\n",
      "Epoch 10, Train Loss: 1.4058, Validation Loss: 1.3755, Validation Accuracy: 0.4360\n",
      "Epoch 11, Train Loss: 1.3969, Validation Loss: 1.3682, Validation Accuracy: 0.4560\n",
      "Epoch 12, Train Loss: 1.3837, Validation Loss: 1.3576, Validation Accuracy: 0.4520\n",
      "Epoch 13, Train Loss: 1.3770, Validation Loss: 1.3557, Validation Accuracy: 0.4680\n",
      "Epoch 14, Train Loss: 1.3664, Validation Loss: 1.3430, Validation Accuracy: 0.4640\n",
      "Epoch 15, Train Loss: 1.3692, Validation Loss: 1.3437, Validation Accuracy: 0.4640\n",
      "Epoch 16, Train Loss: 1.3615, Validation Loss: 1.3341, Validation Accuracy: 0.4680\n",
      "Epoch 17, Train Loss: 1.3475, Validation Loss: 1.3272, Validation Accuracy: 0.4640\n",
      "Epoch 18, Train Loss: 1.3530, Validation Loss: 1.3168, Validation Accuracy: 0.4720\n",
      "Epoch 19, Train Loss: 1.3429, Validation Loss: 1.3077, Validation Accuracy: 0.4800\n",
      "Epoch 20, Train Loss: 1.3297, Validation Loss: 1.3045, Validation Accuracy: 0.4880\n",
      "Epoch 21, Train Loss: 1.3280, Validation Loss: 1.2974, Validation Accuracy: 0.4960\n",
      "Epoch 22, Train Loss: 1.3180, Validation Loss: 1.2930, Validation Accuracy: 0.4840\n",
      "Epoch 23, Train Loss: 1.3107, Validation Loss: 1.2903, Validation Accuracy: 0.4880\n",
      "Epoch 24, Train Loss: 1.3083, Validation Loss: 1.2806, Validation Accuracy: 0.4800\n",
      "Epoch 25, Train Loss: 1.2957, Validation Loss: 1.2733, Validation Accuracy: 0.5120\n",
      "Epoch 26, Train Loss: 1.2918, Validation Loss: 1.2634, Validation Accuracy: 0.4920\n",
      "Epoch 27, Train Loss: 1.2788, Validation Loss: 1.2582, Validation Accuracy: 0.5280\n",
      "Epoch 28, Train Loss: 1.2876, Validation Loss: 1.2590, Validation Accuracy: 0.5000\n",
      "Epoch 29, Train Loss: 1.2708, Validation Loss: 1.2451, Validation Accuracy: 0.5240\n",
      "Epoch 30, Train Loss: 1.2676, Validation Loss: 1.2373, Validation Accuracy: 0.5080\n",
      "Epoch 31, Train Loss: 1.2648, Validation Loss: 1.2404, Validation Accuracy: 0.4920\n",
      "Epoch 32, Train Loss: 1.2582, Validation Loss: 1.2366, Validation Accuracy: 0.5080\n",
      "Epoch 33, Train Loss: 1.2504, Validation Loss: 1.2224, Validation Accuracy: 0.5320\n",
      "Epoch 34, Train Loss: 1.2420, Validation Loss: 1.2263, Validation Accuracy: 0.5240\n",
      "Epoch 35, Train Loss: 1.2343, Validation Loss: 1.2258, Validation Accuracy: 0.5360\n",
      "Epoch 36, Train Loss: 1.2274, Validation Loss: 1.2183, Validation Accuracy: 0.5000\n",
      "Epoch 37, Train Loss: 1.2227, Validation Loss: 1.2117, Validation Accuracy: 0.5120\n",
      "Epoch 38, Train Loss: 1.2259, Validation Loss: 1.2116, Validation Accuracy: 0.5240\n",
      "Epoch 39, Train Loss: 1.2220, Validation Loss: 1.2080, Validation Accuracy: 0.5400\n",
      "Epoch 40, Train Loss: 1.2032, Validation Loss: 1.2027, Validation Accuracy: 0.5120\n",
      "Epoch 41, Train Loss: 1.1986, Validation Loss: 1.1977, Validation Accuracy: 0.5400\n",
      "Epoch 42, Train Loss: 1.2026, Validation Loss: 1.2037, Validation Accuracy: 0.5280\n",
      "Epoch 43, Train Loss: 1.2040, Validation Loss: 1.2050, Validation Accuracy: 0.5360\n",
      "Epoch 44, Train Loss: 1.1873, Validation Loss: 1.1909, Validation Accuracy: 0.5160\n",
      "Epoch 45, Train Loss: 1.1790, Validation Loss: 1.1849, Validation Accuracy: 0.5200\n",
      "Epoch 46, Train Loss: 1.1827, Validation Loss: 1.1823, Validation Accuracy: 0.5120\n",
      "Epoch 47, Train Loss: 1.1763, Validation Loss: 1.1669, Validation Accuracy: 0.5280\n",
      "Epoch 48, Train Loss: 1.1731, Validation Loss: 1.1871, Validation Accuracy: 0.5160\n",
      "Epoch 49, Train Loss: 1.1691, Validation Loss: 1.1794, Validation Accuracy: 0.5320\n",
      "Epoch 50, Train Loss: 1.1656, Validation Loss: 1.1672, Validation Accuracy: 0.5280\n",
      "Epoch 51, Train Loss: 1.1667, Validation Loss: 1.1561, Validation Accuracy: 0.5200\n",
      "Epoch 52, Train Loss: 1.1563, Validation Loss: 1.1725, Validation Accuracy: 0.5400\n",
      "Epoch 53, Train Loss: 1.1561, Validation Loss: 1.1650, Validation Accuracy: 0.5440\n",
      "Epoch 54, Train Loss: 1.1541, Validation Loss: 1.1479, Validation Accuracy: 0.5480\n",
      "Epoch 55, Train Loss: 1.1419, Validation Loss: 1.1557, Validation Accuracy: 0.5440\n",
      "Epoch 56, Train Loss: 1.1465, Validation Loss: 1.1447, Validation Accuracy: 0.5480\n",
      "Epoch 57, Train Loss: 1.1375, Validation Loss: 1.1409, Validation Accuracy: 0.5520\n",
      "Epoch 58, Train Loss: 1.1329, Validation Loss: 1.1406, Validation Accuracy: 0.5520\n",
      "Epoch 59, Train Loss: 1.1363, Validation Loss: 1.1336, Validation Accuracy: 0.5360\n",
      "Epoch 60, Train Loss: 1.1214, Validation Loss: 1.1415, Validation Accuracy: 0.5640\n",
      "Epoch 61, Train Loss: 1.1204, Validation Loss: 1.1330, Validation Accuracy: 0.5480\n",
      "Epoch 62, Train Loss: 1.1168, Validation Loss: 1.1320, Validation Accuracy: 0.5440\n",
      "Epoch 63, Train Loss: 1.1140, Validation Loss: 1.1296, Validation Accuracy: 0.5560\n",
      "Epoch 64, Train Loss: 1.1040, Validation Loss: 1.1129, Validation Accuracy: 0.5440\n",
      "Epoch 65, Train Loss: 1.1179, Validation Loss: 1.1196, Validation Accuracy: 0.5560\n",
      "Epoch 66, Train Loss: 1.0992, Validation Loss: 1.1082, Validation Accuracy: 0.5800\n",
      "Epoch 67, Train Loss: 1.1121, Validation Loss: 1.1123, Validation Accuracy: 0.5520\n",
      "Epoch 68, Train Loss: 1.1007, Validation Loss: 1.1120, Validation Accuracy: 0.5520\n",
      "Epoch 69, Train Loss: 1.0976, Validation Loss: 1.0986, Validation Accuracy: 0.5600\n",
      "Epoch 70, Train Loss: 1.1009, Validation Loss: 1.0980, Validation Accuracy: 0.5360\n",
      "Epoch 71, Train Loss: 1.0800, Validation Loss: 1.0977, Validation Accuracy: 0.5760\n",
      "Epoch 72, Train Loss: 1.0761, Validation Loss: 1.0937, Validation Accuracy: 0.5640\n",
      "Epoch 73, Train Loss: 1.0746, Validation Loss: 1.0953, Validation Accuracy: 0.5800\n",
      "Epoch 74, Train Loss: 1.0776, Validation Loss: 1.0941, Validation Accuracy: 0.5760\n",
      "Epoch 75, Train Loss: 1.0730, Validation Loss: 1.0946, Validation Accuracy: 0.5840\n",
      "Epoch 76, Train Loss: 1.0828, Validation Loss: 1.0910, Validation Accuracy: 0.5680\n",
      "Epoch 77, Train Loss: 1.0689, Validation Loss: 1.0822, Validation Accuracy: 0.5840\n",
      "Epoch 78, Train Loss: 1.0739, Validation Loss: 1.0840, Validation Accuracy: 0.5800\n",
      "Epoch 79, Train Loss: 1.0543, Validation Loss: 1.0761, Validation Accuracy: 0.5880\n",
      "Epoch 80, Train Loss: 1.0510, Validation Loss: 1.0527, Validation Accuracy: 0.5720\n",
      "Epoch 81, Train Loss: 1.0551, Validation Loss: 1.1005, Validation Accuracy: 0.5720\n",
      "Epoch 82, Train Loss: 1.0425, Validation Loss: 1.0862, Validation Accuracy: 0.5760\n",
      "Epoch 83, Train Loss: 1.0373, Validation Loss: 1.0680, Validation Accuracy: 0.5920\n",
      "Epoch 84, Train Loss: 1.0455, Validation Loss: 1.0701, Validation Accuracy: 0.5680\n",
      "Epoch 85, Train Loss: 1.0378, Validation Loss: 1.0633, Validation Accuracy: 0.5720\n",
      "Epoch 86, Train Loss: 1.0380, Validation Loss: 1.0662, Validation Accuracy: 0.5680\n",
      "Epoch 87, Train Loss: 1.0335, Validation Loss: 1.0756, Validation Accuracy: 0.5760\n",
      "Epoch 88, Train Loss: 1.0289, Validation Loss: 1.0596, Validation Accuracy: 0.5720\n",
      "Epoch 89, Train Loss: 1.0296, Validation Loss: 1.0736, Validation Accuracy: 0.5760\n",
      "Epoch 90, Train Loss: 1.0300, Validation Loss: 1.0492, Validation Accuracy: 0.5920\n",
      "Epoch 91, Train Loss: 1.0203, Validation Loss: 1.0791, Validation Accuracy: 0.5760\n",
      "Epoch 92, Train Loss: 1.0107, Validation Loss: 1.0405, Validation Accuracy: 0.5960\n",
      "Epoch 93, Train Loss: 1.0202, Validation Loss: 1.0346, Validation Accuracy: 0.5840\n",
      "Epoch 94, Train Loss: 1.0162, Validation Loss: 1.0798, Validation Accuracy: 0.5600\n",
      "Epoch 95, Train Loss: 1.0133, Validation Loss: 1.0583, Validation Accuracy: 0.5920\n",
      "Epoch 96, Train Loss: 1.0063, Validation Loss: 1.0696, Validation Accuracy: 0.5800\n",
      "Epoch 97, Train Loss: 0.9996, Validation Loss: 1.0494, Validation Accuracy: 0.5960\n",
      "Epoch 98, Train Loss: 1.0023, Validation Loss: 1.0279, Validation Accuracy: 0.5960\n",
      "Epoch 99, Train Loss: 0.9950, Validation Loss: 1.0465, Validation Accuracy: 0.5760\n",
      "Epoch 100, Train Loss: 0.9877, Validation Loss: 1.0270, Validation Accuracy: 0.5800\n",
      "Epoch 101, Train Loss: 0.9990, Validation Loss: 1.0396, Validation Accuracy: 0.5840\n",
      "Epoch 102, Train Loss: 0.9848, Validation Loss: 1.0229, Validation Accuracy: 0.5840\n",
      "Epoch 103, Train Loss: 0.9856, Validation Loss: 1.0530, Validation Accuracy: 0.5800\n",
      "Epoch 104, Train Loss: 0.9752, Validation Loss: 1.0371, Validation Accuracy: 0.6040\n",
      "Epoch 105, Train Loss: 0.9824, Validation Loss: 1.0162, Validation Accuracy: 0.6040\n",
      "Epoch 106, Train Loss: 0.9761, Validation Loss: 1.0541, Validation Accuracy: 0.5720\n",
      "Epoch 107, Train Loss: 0.9595, Validation Loss: 1.0286, Validation Accuracy: 0.5800\n",
      "Epoch 108, Train Loss: 0.9658, Validation Loss: 1.0090, Validation Accuracy: 0.6160\n",
      "Epoch 109, Train Loss: 0.9564, Validation Loss: 1.0226, Validation Accuracy: 0.5960\n",
      "Epoch 110, Train Loss: 0.9796, Validation Loss: 1.0359, Validation Accuracy: 0.6040\n",
      "Epoch 111, Train Loss: 0.9636, Validation Loss: 1.0027, Validation Accuracy: 0.6280\n",
      "Epoch 112, Train Loss: 0.9582, Validation Loss: 1.0475, Validation Accuracy: 0.5920\n",
      "Epoch 113, Train Loss: 0.9607, Validation Loss: 1.0106, Validation Accuracy: 0.6000\n",
      "Epoch 114, Train Loss: 0.9506, Validation Loss: 1.0132, Validation Accuracy: 0.5880\n",
      "Epoch 115, Train Loss: 0.9503, Validation Loss: 0.9972, Validation Accuracy: 0.6200\n",
      "Epoch 116, Train Loss: 0.9466, Validation Loss: 1.0118, Validation Accuracy: 0.6000\n",
      "Epoch 117, Train Loss: 0.9422, Validation Loss: 1.0050, Validation Accuracy: 0.6000\n",
      "Epoch 118, Train Loss: 0.9411, Validation Loss: 1.0465, Validation Accuracy: 0.6000\n",
      "Epoch 119, Train Loss: 0.9362, Validation Loss: 1.0102, Validation Accuracy: 0.6200\n",
      "Epoch 120, Train Loss: 0.9386, Validation Loss: 1.0011, Validation Accuracy: 0.6200\n",
      "Epoch 121, Train Loss: 0.9483, Validation Loss: 1.0069, Validation Accuracy: 0.6120\n",
      "Epoch 122, Train Loss: 0.9325, Validation Loss: 0.9942, Validation Accuracy: 0.6240\n",
      "Epoch 123, Train Loss: 0.9253, Validation Loss: 1.0237, Validation Accuracy: 0.5920\n",
      "Epoch 124, Train Loss: 0.9262, Validation Loss: 1.0051, Validation Accuracy: 0.6000\n",
      "Epoch 125, Train Loss: 0.9315, Validation Loss: 1.0392, Validation Accuracy: 0.5960\n",
      "Epoch 126, Train Loss: 0.9264, Validation Loss: 1.0124, Validation Accuracy: 0.6080\n",
      "Epoch 127, Train Loss: 0.9129, Validation Loss: 1.0143, Validation Accuracy: 0.6000\n",
      "Epoch 128, Train Loss: 0.9168, Validation Loss: 1.0114, Validation Accuracy: 0.5960\n",
      "Epoch 129, Train Loss: 0.9219, Validation Loss: 1.0343, Validation Accuracy: 0.5920\n",
      "Epoch 130, Train Loss: 0.9136, Validation Loss: 1.0064, Validation Accuracy: 0.6200\n",
      "Epoch 131, Train Loss: 0.9215, Validation Loss: 1.0212, Validation Accuracy: 0.6000\n",
      "Epoch 132, Train Loss: 0.9181, Validation Loss: 1.0021, Validation Accuracy: 0.6080\n",
      "Epoch 133, Train Loss: 0.9022, Validation Loss: 0.9824, Validation Accuracy: 0.6240\n",
      "Epoch 134, Train Loss: 0.8856, Validation Loss: 1.0030, Validation Accuracy: 0.6120\n",
      "Epoch 135, Train Loss: 0.8976, Validation Loss: 1.0019, Validation Accuracy: 0.6160\n",
      "Epoch 136, Train Loss: 0.8951, Validation Loss: 1.0277, Validation Accuracy: 0.6160\n",
      "Epoch 137, Train Loss: 0.9036, Validation Loss: 0.9845, Validation Accuracy: 0.6280\n",
      "Epoch 138, Train Loss: 0.9147, Validation Loss: 0.9888, Validation Accuracy: 0.6200\n",
      "Epoch 139, Train Loss: 0.8979, Validation Loss: 1.0498, Validation Accuracy: 0.6000\n",
      "Epoch 140, Train Loss: 0.8801, Validation Loss: 0.9827, Validation Accuracy: 0.6040\n"
     ]
    }
   ],
   "source": [
    "models, optimizers, criterions = get_models(gModel_names, gOptimizer_names, gCriterion_names)\n",
    "\n",
    "for m in range(gnDataset):\n",
    "    model = models[m]\n",
    "    optimizer = optimizers[m]\n",
    "    criterion = criterions[m]\n",
    "    train_dataloader, test_dataloader = load_data(train_data_paths[m], test_data_paths[m], m)\n",
    "\n",
    "\n",
    "    progress = CAS771Plot(model, criterion, device, train_dataloader, test_dataloader, m)\n",
    "    es = CAS771EarlyStopping()\n",
    "\n",
    "    for epoch in range(gnEpochs):\n",
    "        model.train()\n",
    "        progress.init_running_loss()\n",
    "\n",
    "        for inputs, labels in train_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) # move data to GPU/CPU\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            progress.add_loss(loss.item())\n",
    "\n",
    "        if es.isStop(progress.append(epoch)):\n",
    "            break\n",
    "    progress.plot()\n",
    "    save_model(model, m, gSaveModelAll)\n",
    "\n",
    "    del model\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JFqA8NF6rvfk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFBRqSizrvfk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "5\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
