{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "gnDataset = 3\n",
    "gBase_path = 'CAS771/Task1_data'\n",
    "gnClasses = 5\n",
    "\n",
    "gDropout_rate = [0.5, 0.5, 0.5]\n",
    "gBatch_momentum = [0.2, 0.2, 0.2]\n",
    "gLearning_rate = [0.00001, 0.00001, 0.00001]\n",
    "gWeight_decay = [1e-3, 1e-3, 1e-3]\n",
    "gOpt_Momentum = [0.9, 0.9, 0.9]\n",
    "\n",
    "gData_batch_size = 64\n",
    "gEarly_stop_patience = 15\n",
    "gnEpochs = 100\n",
    "gbData_augmentation = True\n",
    "\n",
    "gModel_names = [\"cnn\", \"cnn\", \"resnet\"] # \"resnet\", \"cnn\"\n",
    "gOptimizer_names = [\"Adam\", \"Adam\", \"Adam\"] # \"Adam\", \"SGD\"\n",
    "gCriterion_names = [\"CrossEntropyLoss\", \"CrossEntropyLoss\", \"CrossEntropyLoss\"]\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ Widget functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "\n",
    "class CAS771Dataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform and not isinstance(img, torch.Tensor):\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "def _load_data(train_data_path):\n",
    "    raw_data = torch.load(train_data_path)\n",
    "    data = raw_data['data']\n",
    "    labels = raw_data['labels']\n",
    "    return data, labels\n",
    "\n",
    "def remap_labels(labels, class_mapping):\n",
    "    return [class_mapping[label] for label in labels]\n",
    "\n",
    "def load_class_names(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        classes = [line.strip() for line in file]\n",
    "    return classes\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def count_layers(model):\n",
    "    layer_count = 0\n",
    "    for module in model.children():\n",
    "        if not isinstance(module, nn.Dropout):  # deduct Dropout\n",
    "            layer_count += 1\n",
    "    return layer_count\n",
    "\n",
    "\n",
    "class CAS771Plot():\n",
    "    def __init__(self, model, criterion, device, train_dataloader, test_dataloader, model_num):\n",
    "        self.model = model\n",
    "        self.criterion = criterion \n",
    "        self.device = device\n",
    "        self.train_losses = []\n",
    "        self.validation_losses = []\n",
    "        self.validation_accuracies = []\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.running_loss = 0.0\n",
    "        self.model_num = model_num + 1\n",
    "    \n",
    "    def init_running_loss(self):\n",
    "        self.running_loss = 0.0\n",
    "\n",
    "    def add_loss(self, loss):\n",
    "        self.running_loss += loss\n",
    "\n",
    "    def append(self, epoch):\n",
    "        train_loss = self.running_loss / len(self.train_dataloader)\n",
    "        validation_loss, validation_accuracy = self._validate()\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.validation_losses.append(validation_loss)\n",
    "        self.validation_accuracies.append(validation_accuracy)\n",
    "        self._print(epoch, train_loss, validation_loss, validation_accuracy)\n",
    "        return validation_loss\n",
    "\n",
    "    def plot(self):\n",
    "        self._plot_metrics(self.train_losses, self.validation_losses, self.validation_accuracies, self.model_num)\n",
    "    \n",
    "    def _print(self, epoch, train_loss, validation_loss, validation_accuracy):\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}\")\n",
    "\n",
    "    def _validate(self):\n",
    "        self.model.eval()  # evaluation mode\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():  # disable gradient calculations\n",
    "            for inputs, labels in self.test_dataloader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_samples += labels.size(0)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "        \n",
    "        validation_loss = running_loss / len(self.test_dataloader)\n",
    "        validation_accuracy = correct_predictions / total_samples\n",
    "        return validation_loss, validation_accuracy\n",
    "\n",
    "    def _plot_metrics(self, train_losses, validation_losses, validation_accuracies, model_num):\n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
    "        plt.plot(epochs, validation_losses, label='Validation Loss', marker='o')\n",
    "        plt.title(f'Model {model_num} Learning per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, validation_accuracies, label='Validation Accuracy', marker='o', color='green')\n",
    "        plt.title(f'Model {model_num} Accuracy per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()    \n",
    "\n",
    "\n",
    "class CAS771EarlyStopping():\n",
    "    def __init__(self):\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "\n",
    "    def isStop(self, validation_loss):\n",
    "        if validation_loss < self.best_val_loss: # 검증 손실이 감소하면 best_val_loss 업데이트\n",
    "            self.best_val_loss = validation_loss\n",
    "            self.patience_counter = 0\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "        \n",
    "        if self.patience_counter >= gEarly_stop_patience: # 검증 손실이 감소하지 않으면 학습 종료\n",
    "            print(\"Early stopping\")\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def get_data_augmentation(mode):\n",
    "    if gbData_augmentation == False:\n",
    "        return None\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        return transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomHorizontalFlip(), # inverse left-right\n",
    "            transforms.RandomRotation(degrees=15), # random rotate\n",
    "            transforms.RandomResizedCrop(32, scale=(0.8, 1.0)), # random crop\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # normalization\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "def load_data(train_data_path, test_data_path, m=0, save_class_mapping=True):\n",
    "    train_data, train_labels = _load_data(train_data_path)\n",
    "    unique_labels = sorted(set(train_labels))\n",
    "    class_mapping = {label: i for i, label in enumerate(unique_labels)}\n",
    "    print(f\"Class mapping: {class_mapping}\")\n",
    "    \n",
    "    if train_data_path == None:\n",
    "        train_dataloader = None\n",
    "    else:\n",
    "        train_remapped_labels = remap_labels(train_labels, class_mapping)\n",
    "        transform = get_data_augmentation(\"train\")\n",
    "        train_dataset = CAS771Dataset(train_data, train_remapped_labels, transform=transform)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=gData_batch_size, shuffle=True)\n",
    "\n",
    "        if save_class_mapping:\n",
    "            # Save the mapping to a file\n",
    "            class_mapping_path = f'{gBase_path}/Model{m+1}/class_mapping_model_{m+1}.pkl'\n",
    "            with open(class_mapping_path, \"wb\") as f:\n",
    "                pickle.dump(class_mapping, f)\n",
    "\n",
    "    if test_data_path == None:\n",
    "        test_dataloader = None\n",
    "    else:\n",
    "        test_data, test_labels = _load_data(test_data_path)\n",
    "        remapped_test_labels = remap_labels(test_labels, class_mapping)\n",
    "        transform = get_data_augmentation(\"test\")\n",
    "        test_dataset = CAS771Dataset(test_data, remapped_test_labels, transform=transform)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=gData_batch_size, shuffle=False)\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CAS771/Task1_data/Model1/model1_train.pth', 'CAS771/Task1_data/Model2/model2_train.pth', 'CAS771/Task1_data/Model3/model3_train.pth']\n",
      "['CAS771/Task1_data/Model1/model1_test.pth', 'CAS771/Task1_data/Model2/model2_test.pth', 'CAS771/Task1_data/Model3/model3_test.pth']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'CAS771/Task1_data/cifar100_classes.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_data_paths)\n\u001b[1;32m      6\u001b[0m classes_path \u001b[38;5;241m=\u001b[39m gBase_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/cifar100_classes.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[43mload_class_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclasses_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(classes)\n",
      "Cell \u001b[0;32mIn[2], line 36\u001b[0m, in \u001b[0;36mload_class_names\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_class_names\u001b[39m(filepath):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     37\u001b[0m         classes \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file]\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m classes\n",
      "File \u001b[0;32m~/myenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CAS771/Task1_data/cifar100_classes.txt'"
     ]
    }
   ],
   "source": [
    "train_data_paths = [f'{gBase_path}/Model{i}/model{i}_train.pth' for i in range(1, gnDataset+1)]\n",
    "test_data_paths = [f'{gBase_path}/Model{i}/model{i}_test.pth' for i in range(1, gnDataset+1)]\n",
    "print(train_data_paths)\n",
    "print(test_data_paths)\n",
    "\n",
    "classes_path = gBase_path + '/cifar100_classes.txt'\n",
    "classes = load_class_names(classes_path)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Basic Block (Residual Block)\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, dropout_rate=0.0, batch_momentum=0.1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        # 1st Convolution Layer\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels, momentum=batch_momentum)\n",
    "        \n",
    "        # 2nd Convolution Layer\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels, momentum=batch_momentum)\n",
    "        \n",
    "        # Skip connection (identity)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels, momentum=batch_momentum)\n",
    "            )\n",
    "\n",
    "        # Dropout Layer (After Batch Normalization)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Residual connection + ReLU activation\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)  # skip connection\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "# Define Modified ResNet-18\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.5, batch_momentum=0.1):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.in_channels = 32  \n",
    "        # 1st Layer (Conv + Maxpool)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3, bias=False) \n",
    "        self.bn1 = nn.BatchNorm2d(32, momentum=batch_momentum)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # Residual Blocks (each block is BasicBlock)\n",
    "        #self.layer0 = self._make_layer(64, 2, stride=1, dropout_rate=dropout_rate/2)   \n",
    "        self.layer1 = self._make_layer(64, 2, \n",
    "                                       stride=1, dropout_rate=dropout_rate/2,\n",
    "                                       batch_momentum=batch_momentum)   \n",
    "        self.layer2 = self._make_layer(128, 2, stride=2, \n",
    "                                       dropout_rate=dropout_rate/2,\n",
    "                                       batch_momentum=batch_momentum) \n",
    "        self.layer3 = self._make_layer(256, 2, stride=2, dropout_rate=dropout_rate/2,\n",
    "                                       batch_momentum=batch_momentum) \n",
    "        self.layer4 = self._make_layer(256, 2, stride=2, dropout_rate=dropout_rate/2,\n",
    "                                       batch_momentum=batch_momentum) \n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(256, num_classes) \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride, dropout_rate, batch_momentum):\n",
    "        layers = []\n",
    "        layers.append(BasicBlock(self.in_channels, \n",
    "                                 out_channels, \n",
    "                                 stride, \n",
    "                                 dropout_rate=dropout_rate,\n",
    "                                 batch_momentum=batch_momentum))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(BasicBlock(self.in_channels, out_channels, dropout_rate=dropout_rate))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1st Convolution + Maxpool\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        # Residual Blocks\n",
    "        #x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # Fully Connected\n",
    "        x = torch.flatten(x, 1)  # Flatten the tensor\n",
    "        x = self.dropout(x)      # Apply Dropout\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.5):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Pooling layers\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Calculate the flattened size by passing dummy input\n",
    "        dummy_input = torch.randn(1, 3, 32, 32)\n",
    "        dummy_output = self.pool2(F.relu(self.bn3(self.conv3(self.pool1(F.relu(self.bn2(self.conv2(F.relu(self.bn1(self.conv1(dummy_input)))))))))))\n",
    "        flattened_size = dummy_output.view(1, -1).size(1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(flattened_size, 465)\n",
    "        self.fc2 = nn.Linear(465, num_classes)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        # Convolutional layer + ReLU + Batch Normalization\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x) # first pooling\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool2(x) # second pooling\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers + Dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_model(model_name, dropout_rate, batch_momentum, m = 0):\n",
    "    if model_name == \"resnet\":\n",
    "        model = ResNet18(num_classes=gnClasses, \n",
    "                        dropout_rate=dropout_rate, \n",
    "                        batch_momentum=batch_momentum).to(device)\n",
    "    elif model_name == \"cnn\":\n",
    "        model = CNN(num_classes=gnClasses,\n",
    "                     dropout_rate=dropout_rate).to(device)\n",
    "\n",
    "\n",
    "    print(f\"Model{m+1} parameters: {count_parameters(model)}\")\n",
    "    print(f\"Model{m+1} Total layers: {count_layers(model)}\")\n",
    "    return model\n",
    "\n",
    "def _get_optimizer(opname, model, learning_rate, weight_decay, opt_momentum):\n",
    "    if opname == \"Adam\":\n",
    "        return optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif opname == \"SGD\":\n",
    "        return optim.SGD(model.parameters(), lr=learning_rate, momentum = opt_momentum, weight_decay=weight_decay)\n",
    "\n",
    "def _get_criterion(criterion_name):\n",
    "    if criterion_name == \"CrossEntropyLoss\":\n",
    "        return nn.CrossEntropyLoss()\n",
    "    return nn.CrossEntropyLoss()\n",
    "\n",
    "def get_models(model_names, optimizer_names, criterion_names):\n",
    "    models = []\n",
    "    optimizers = []\n",
    "    criterions = []\n",
    "    for m in range(gnDataset):\n",
    "        models.append(_get_model(model_names[m], gDropout_rate[m], gBatch_momentum[m], m))\n",
    "        optimizers.append(_get_optimizer(optimizer_names[m], models[m], gLearning_rate[m], gWeight_decay[m], gOpt_Momentum[m]))\n",
    "        criterions.append(_get_criterion(criterion_names[m]))\n",
    "    \n",
    "    return models, optimizers, criterions\n",
    "\n",
    "    \n",
    "def save_model(model, m=0):\n",
    "    model_saved_path = f\"{gBase_path}/Model{m+1}/model{m+1}_weights.pth\"\n",
    "    torch.save(model.state_dict(), model_saved_path)\n",
    "    print(f\"Model{m+1} is saved to: {model_saved_path}\")\n",
    "\n",
    "def load_model(model, m=0):\n",
    "    model_saved_path = f\"{gBase_path}/Model{m+1}/model{m+1}_weights.pth\"\n",
    "    model.load_state_dict(torch.load(model_saved_path))\n",
    "    print(f\"Model{m+1} is loaded from: {model_saved_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ Training / Validating for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, optimizers, criterions = get_models(gModel_names, gOptimizer_names, gCriterion_names)\n",
    "\n",
    "for m in range(gnDataset):\n",
    "    model = models[m]\n",
    "    optimizer = optimizers[m]\n",
    "    criterion = criterions[m]\n",
    "    train_dataloader, test_dataloader = load_data(train_data_paths[m], test_data_paths[m], m)\n",
    "\n",
    "    progress = CAS771Plot(model, criterion, device, train_dataloader, test_dataloader, m)\n",
    "    es = CAS771EarlyStopping()\n",
    "\n",
    "    for epoch in range(gnEpochs):\n",
    "        model.train()\n",
    "        progress.init_running_loss()\n",
    "\n",
    "        for inputs, labels in train_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) # move data to GPU/CPU\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            progress.add_loss(loss.item())\n",
    "        \n",
    "        if es.isStop(progress.append(epoch)):\n",
    "            break\n",
    "    progress.plot()\n",
    "    save_model(model, m)\n",
    "\n",
    "    del model\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
